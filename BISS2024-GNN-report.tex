% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{url}
%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{acronym}
\usepackage[inline]{enumitem}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{dblfloatfix}
% MUST BE THE LAST IMPORT
\usepackage{cleveref}


\acrodef{gnn}[GNN]{Graph Neural Network}

\newcommand{\meta}[1]{{\color{blue}#1}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Graph Neural Network Project \\
Bertinoro International Spring School (BISS) 2024}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Martina Baiardi \and Davide Domini \and Nicolas Farabegoli \and Alessandro Petrella \and Gianni Tumedei }
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


\author{
  Martina Baiardi \\
  University of Bologna \\
  {\bf m.baiardi@unibo.it} \\ \And
  Davide Domini \\
  University of Bologna \\
  {\bf davide.domini@unibo.it} \\  \And
  Nicolas Farabegoli \\
  University of Bologna \\
  {\bf nicolas.farabegoli@unibo.it} \\  \AND
  Alessandro Petrella\\
  University of Bologna \\
  {\bf alessandro.petrella@unibo.it} \\ \And
  Gianni Tumedei \\
  University of Bologna \\
  {\bf gianni.tumedei2@unibo.it}
}

\begin{document}

\maketitle

\begin{abstract}

Online platforms for products and services - such as Spotify, Netflix, and Amazon - 
host vast product inventories and cater to millions of users. 
Therefore, it is essential to implement an optimized automated system that effectively 
recommends products aligned with users' tastes and preferences.  
%
\acp{gnn} show great promise for this purpose, and in this paper, 
we introduce a technique to develop a recommender system based on 
link prediction in heterogeneous graphs, using the MovieLens 100K dataset.
%
We provide details about the adopted architecture, the model configuration and the training
process followed in our experiments.


\end{abstract}

\section{Task description}\label{sec:task-description}
Link prediction tasks involve predicting the existence, formation, 
or strength of a connection (or link) between two nodes in a network or graph. 
These tasks are crucial in various domains where understanding or forecasting the relationships 
between entities is important. 
In this paper, we present a recommendation system that exploits link prediction to recommend movies to its users.
The goal is to predict whether a user is likely to enjoy a film they haven't yet seen, 
enabling the system to recommend it to them, or exclude it if it's unlikely to match their preferences.


\section{Dataset description}\label{sec:dataset-description}
The Movielens dataset \cite{harper2015movielens} is a widely used benchmark dataset in the field of recommender systems and machine learning research. 
Released by the GroupLens Research lab, the dataset contains around 100,000 ratings from 610 users on 9,742 movies, 
captured between March 29, 1996, and September 24, 2018. 
Each user in the dataset has rated at least 20 movies, with ratings ranging from 1 to 5, where 1 indicates the lowest rating and 5 the highest.
Each user is represented by an ID, and no other information is provided about them, while movies have additional information such as title, genres, and release year. Notably, the genres of a movie are all contained in a single string, and use a pipe character ``\texttt{|}'' as a separator. Furthermore, the year of release is stored between parentheses at the end of the movie title and is absent in some movies.
The dataset is structured into the following files: \texttt{links.csv}, \texttt{movies.csv}, \texttt{ratings.csv}, and \texttt{tags.csv}.
The Movielens dataset is particularly valuable for researchers because of its size.

\begin{figure*}[b]
  \centering
  \includegraphics[width=1\linewidth]{figures/architecture.pdf}

  \caption{
    Architecture
  }
  \label{fig:architecture}
\end{figure*}

\section{Related Work}\label{sec:related-work}

Link prediction and recommender systems are interrelated fields that focus, respectively, on predicting missing links in networks 
and recommending items to users. Early link prediction approaches relied on measuring the similarity 
between pairs of nodes using metrics such as Common Neighbors, Jaccard Coefficient, and the Adamic-Adar index 
\cite{liben2003link}. These heuristics exploit the topology of the network to estimate the likelihood 
of a link's existence based on local neighborhood structures.

Probabilistic models and matrix factorization techniques were later introduced to capture more complex 
patterns in network structures \cite{koren2009matrix}. With the rise of deep learning, \acp{gnn} have become 
prominent for link prediction tasks. Methods like Graph Convolutional Networks (GCNs) and 
Graph Autoencoders can learn node embeddings that encapsulate both local and global network information, 
leading to improved prediction accuracy \cite{kipf2016semi}.

Recommender systems have evolved from traditional collaborative filtering techniques to advanced models 
incorporating deep learning and network information. Collaborative filtering methods, such as user-based 
and item-based approaches, utilize user-item interaction matrices to suggest relevant items. 
Matrix factorization methods, like Singular Value Decomposition (SVD), project users and items into a latent 
feature space, effectively handling large-scale recommendation tasks \cite{sarwar2001item}.

The integration of social network data into recommender systems has led to the development of social recommender 
systems, which leverage social relationships to enhance recommendation accuracy \cite{jamali2010matrix}. 
Recent advancements have seen the application of \acp{gnn}s in recommender systems, enabling the modeling of high-order 
connectivity and complex user-item interactions \cite{wang2019neural}. These models capture the intricate structures 
within user-item graphs, leading to significant improvements in recommendation performance.

The convergence of link prediction and recommender systems is evident in the utilization of network embedding 
techniques and GNNs to model interactions and predict missing links in various networks. This synergy has opened new 
research avenues focusing on enhancing prediction accuracy, scalability, and adaptability in large and dynamic networks.

\section{Architecture overview}\label{sec:architecture-overview}

This section provides an high-level description of the adopted architecture (\Cref{fig:architecture}).
For the link prediction task, which involves a network or a graph for describing data, the first choice is to adopt a \ac{gnn} which takes heterogeneous graphs as input. 

Heterogeneous graphs consist of different types of nodes and edges, 
unlike homogeneous graphs where all nodes and edges are uniform and have the same properties.
In our case, the heterogeneous graph particularly fits the dataset information, 
as we have two types of nodes: users and movies.
A user and a movie node can have a relationship between each other, representing that the user has reviewed the movie, and we named it "user-rates-movie".

Before feeding the graph into the \acp{gnn}, pre-processing is performed to 
generate node embeddings.
These embeddings capture both the structural and semantic features of the graph.
We split movie genres in 20 separate features (one for each genre in the dataset). Each movie of a certain genre will have 1 corresponding to that genre's feature, or 0 if it doesn't belong to the genre.
As for the year, we used a regular expression to extract it from parentheses and use it as a feature.

The \acp{gnn} architecture is composed of \ac{gnn} layers, each one followed by a ReLU activation 
function to introduce non-linearity. The final layer is a binary classifier 
aimed to predict the presence or absence of edges, enabling link prediction within the 
graph.

The most performing architecture is then chosen among different combination of number of \ac{gnn} layers and their type, described in \Cref{sec:experimental-setup}.

\section{Experimental setup}\label{sec:experimental-setup}

The model we developed is structured in two main components. 
%
The first one focuses on generating embeddings from the graph, 
  utilizing a Graph Neural Network (GNN) architecture to capture the necessary features. 
%  
The second part is a classification module that takes these embeddings and predicts the links 
  between nodes within the graph.

In terms of training, we explored various configurations to optimize the model's performance.
%
We began by testing different types of GNN layers, including \texttt{GATv2Conv}, \texttt{SAGEConv}, and \texttt{GraphConv}. 
%
For each of these types, we further experimented with a varying the number of convolutional layers, 
  specifically testing models with 1, 2, 3, and 4 layers to assess their impact on the results.

\section{Results}\label{sec:resutls}

Among the three layers analyzed, \texttt{GraphConv} is the simplest. 
%
Its primary operation consists of averaging the features of neighboring nodes followed by the application of a linear transformation. 
%
From the results (\Cref{tab:results}), it can be observed that training with this layer leads to the most unstable performance (\Cref{fig:graph2}) among all 
%
the experiments conducted, with the lowest AUC scores as well.

In contrast, \texttt{SAGEConv} and \texttt{GATv2Conv} represent more advanced layer types. 
%
The results demonstrate that both layers achieve comparable AUC scores. 
%
However, \texttt{SAGEConv} shows a slightly higher AUC and tends to be more stable during the training process (\Cref{fig:sage2}).

When considering the impact of the number of layers, \texttt{SAGEConv} remains stable as the number of 
%
layers increases, but the gains in AUC are minimal. 
%
This suggests that it is possible to find a balance between model performance and complexity by using only two layers. 
%
On the other hand, \texttt{GATv2Conv} performs better with fewer layers, but its performance degrades when the number 
%
of layers is increased to three or four (\Cref{fig:gat2,fig:gat4}).

\begin{figure*}
  \centering
  \begin{subfigure}{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/charts/Graph/auc-layerType_GraphConv-numLayers_2.pdf}
      \caption{GraphConv with 2 layers.}
      \label{fig:graph2}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/charts/Sage/auc-layerType_SAGEConv-numLayers_2.pdf}
    \caption{SAGEConv with 2 layers.}
    \label{fig:sage2}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/charts/GAT/auc-layerType_GATv2Conv-numLayers_2.pdf}
    \caption{GATv2Conv with 2 layers.}
    \label{fig:gat2}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/charts/GAT/auc-layerType_GATv2Conv-numLayers_4.pdf}
    \caption{GATv2Conv with 4 layers.}
    \label{fig:gat4}
  \end{subfigure}
  \caption{AUC score during training.}
  \label{fig:metrics}
\end{figure*}

\section{Reproducibility}
All the experiments conducted are available 
%
online\footnote{\url{https://tinyurl.com/GNN-BISS-Project}} 
%
and can be executed by anyone. 
%
While we have included only a selection of charts in this report, running the 
%
experiments will automatically generate and export all charts to the \texttt{charts} folder.

\begin{table}
  \centering
  \begin{tabular}{|l|c|l|} 
  \hline
  \textbf{Layer Type}                  & \multicolumn{1}{l|}{\textbf{Number of Layers}} & \textbf{AUC}  \\ 
  \hline
  \multirow{4}{*}{\texttt{GraphConv }} & 1                                              & 0.83          \\ 
  \cline{2-3}
                                       & 2                                              & 0.79          \\ 
  \cline{2-3}
                                       & 3                                              & 0.75          \\ 
  \cline{2-3}
                                       & 4                                              & 0.60          \\ 
  \hline
  \multirow{4}{*}{\texttt{SAGEConv}}   & 1                                              & 0.90          \\ 
  \cline{2-3}
                                       & 2                                              & 0.93          \\ 
  \cline{2-3}
                                       & 3                                              & 0.93          \\ 
  \cline{2-3}
                                       & 4                                              & 0.94          \\ 
  \hline
  \multirow{4}{*}{\texttt{GATv2Conv}}  & 1                                              & 0.91          \\ 
  \cline{2-3}
                                       & 2                                              & 0.91          \\ 
  \cline{2-3}
                                       & 3                                              & 0.86          \\ 
  \cline{2-3}
                                       & 4                                              & 0.90          \\
  \hline
  \end{tabular}

  \caption{Test results for each experiment.}
  \label{tab:results}
\end{table}


\bibliography{bibliography}


% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}
